{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "* pip install lightgbm\n",
    "* conda install -c conda-forge xgboost\n",
    "* pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stack OverFlow: EDA & ML**  \n",
    "\n",
    "**Elijah Zolduoarrati**  \n",
    "**Approaches and Techniques:**\n",
    "\n",
    "* EDA with Pandas and Seaborn\n",
    "* Find features with strong correlation to target variables questions and answers\n",
    "* Data preprocessing, converting categorical features mainly (country) to numerical\n",
    "* apply the basic Regression models of sklearn \n",
    "* use gridsearchCV to find the best parameters for each model\n",
    "* compare the performance of the Regressors and choose best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The notebook is organized as follows:**\n",
    "\n",
    "* **[Part 0: Imports, Settings and switches, Global functions](#Part-0-:-Imports,-Settings,-Functions)**  \n",
    "* import libraries  \n",
    "* settings for number of cross validations  \n",
    "* define functions that are used often\n",
    "\n",
    "* **[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)**  \n",
    "1.1 Get an overview of the features (numerical and categorical) and first look on the target variables questions and answers\n",
    "[shape, info, head and describe](#shape,-info,-head-and-describe)  \n",
    "[Distribution of the target variable Q](#The-target-variable-:-Distribution-of-questions-and-answers)  \n",
    "[Numerical and Categorical features](#Numerical-and-Categorical-features)  \n",
    "[List of features with missing values](#List-of-features-with-missing-values) and Filling missing values using [log transform](#log-transform)  \n",
    "1.2 Relation of all features to target questions and answers  \n",
    "[Seaborn regression plots for numerical features](#Plots-of-relation-to-target-for-all-numerical-features)  \n",
    "[List of numerical features and their correlation coefficient to target](#List-of-numerical-features-and-their-correlation-coefficient-to-target)  \n",
    "[Seaborn boxplots for categorical features](#Relation-to-questions-and-answers-for-all-categorical-features)  \n",
    "[List of categorical features and their unique values](#List-of-categorical-features-and-their-unique-values)  \n",
    "1.3 Determine the columns that show strong correlation to target  \n",
    "[Correlation matrix 1](#Correlation-matrix-1) : all numerical features determine features with largest correlation to questions and answers\n",
    "\n",
    "* **[Part 2: Data wrangling](#Part-2:-Data-wrangling)**  \n",
    "[Dropping all columns with weak correlation to questions and answers](#Dropping-all-columns-with-weak-correlation-to-questions-and-answers)  \n",
    "[Convert categorical columns to numerical](#Convert-categorical-columns-to-numerical)  \n",
    "[Checking correlation to SalePrice for the new numerical columns](#Checking-correlation-to-questions-and-answers-for-the-new-numerical-columns)  \n",
    "use only features with strong correlation to target  \n",
    "[Correlation Matrix 2 (including converted categorical columns)](#Correlation-Matrix-2-:-All-features-with-strong-correlation-to-questions-and-answers)  \n",
    "Create datasets for ML algorithms:                                                                          \n",
    "[OneHotEncoder](#OneHotEncoder)  \n",
    "[StandardScaler](#StandardScaler)\n",
    "\n",
    "* **[Part 3: Scikit-learn basic regression models and comparison of results](#Part-3:-Scikit-learn-basic-regression-models-and-comparison-of-results)**  \n",
    "implement GridsearchCV with RMSE metric for Hyperparameter tuning for these models from sklearn:  \n",
    "[Linear Regression](#Linear-Regression)  \n",
    "[Ridge](#Ridge)  \n",
    "[Lasso](#Lasso)  \n",
    "[Elastic Net](#Elastic-Net)  \n",
    "[Stochastic Gradient Descent](#SGDRegressor)  \n",
    "[DecisionTreeRegressor](#DecisionTreeRegressor)  \n",
    "[Random Forest Regressor](#RandomForestRegressor)  \n",
    "[KNN Regressor](#KNN-Regressor)  \n",
    "Baed on RMSE metric, compare performance of the regressors with their optimized parameters, then explore correlation of the predictions and make submission with mean of best models plot comparison:             \n",
    "[RMSE of all models](#Comparison-plot:-RMSE-of-all-models)  \n",
    "[Correlation of model results](#Correlation-of-model-results)  \n",
    "Mean of best models\n",
    "\n",
    "\n",
    "Note on scores:  \n",
    "Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed questions and answers. (Taking logs means that errors in predicting questions and answers will affect the result equally.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 : Imports, Settings, Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Lib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Math Lib for some statistics\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# df preprocessing Lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('max_columns', 105)\n",
    "\n",
    "# AI preprocessing lib\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# ML Lib\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "# warning supressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#importing necessary models and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings and switches**\n",
    "\n",
    "**Here we can choose settings for optimal performance and runtime. For example, nr_cv sets the number of cross validations used in GridsearchCV, and min_val_corr is the minimum value for the correlation coefficient to the target (only features with larger correlation will be used).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of cross validations used in the Model part \n",
    "nr_cv = 5\n",
    "\n",
    "# switch for using log values for SalePrice and features     \n",
    "use_logvals = 1    \n",
    "# target used for correlation \n",
    "target_1 = 'questions'\n",
    "target_2 = 'answers'    \n",
    "# only columns with correlation above this threshold value  \n",
    "# are used for the ML Regressors in Part 3\n",
    "min_val_corr = 0.4    \n",
    "    \n",
    "# switch for dropping columns that are similar to others already used and show a high correlation to these     \n",
    "drop_similar = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_score(grid):\n",
    "    \n",
    "    best_score = np.sqrt(-grid.best_score_)\n",
    "    print(best_score)    \n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_estimator_)\n",
    "    return best_score\n",
    "\n",
    "def print_cols_large_corr(df, nr_c, targ) :\n",
    "    corr = df.corr()\n",
    "    corr_abs = corr.abs()\n",
    "    print (corr_abs.nlargest(nr_c, targ)[targ])\n",
    "\n",
    "def plot_corr_matrix(df, nr_c, targ) :\n",
    "    \n",
    "    corr = df.corr()\n",
    "    corr_abs = corr.abs()\n",
    "    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n",
    "    cm = np.corrcoef(df[cols].values.T)\n",
    "\n",
    "    plt.figure(figsize=(nr_c/1.5, nr_c/1.5))\n",
    "    sns.set(font_scale=1.25)\n",
    "    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n",
    "                fmt='.2f', annot_kws={'size': 10}, \n",
    "                yticklabels=cols.values, xticklabels=cols.values\n",
    "               )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output, call \n",
    "print(check_output([\"dir\", \"C:\\\\Users\\\\alamo248\\\\Downloads\\\\Data\\\\all_uptt.csv\"],shell=True).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data into dataframe\n",
    "df =  pd.read_csv('C://Data/all_upt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtitute missing values by zero\n",
    "df = df.fillna(0)\n",
    "#Displaying modified dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a variable vector\n",
    "vec = df.iloc[:,[0,6,7,8,9,10,11,12,13,14,15,16,17]]\n",
    "# Or\n",
    "# vec = df.loc[:,['Id','DisplayName','Location','country','AboutMe_length','activity_in_months','UpVotes','DownVotes','Reputation','Views','badges','Q_comments','A_comments','P_questions','P_answers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets\n",
    "vec_train,vec_test= train_test_split(vec, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Overview of features and relation to target\n",
    "\n",
    "Let's get a first overview of the train and test dataset\n",
    "* How many rows and columns are there?  \n",
    "* What are the names of the features (columns)?  \n",
    "* Which features are numerical, which are categorical?  \n",
    "* How many values are missing?  \n",
    "The **shape** and **info** methods answer these questions. Whereas, the **head** displays some rows of the dataset **describe** gives a summary of the statistics (only for numerical columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the independent variable vector\n",
    "x = df.iloc[:,6:15].values\n",
    "# Question dependent variable vector \n",
    "y = df.iloc[:, -2:-1].values\n",
    "# Answer dependent variable vector \n",
    "z = df.iloc[:,-1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labelEncoder object to transform categorical values into integers\n",
    "x[:, 0] = LabelEncoder().fit_transform(x[:, 0])\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "z = LabelEncoder().fit_transform(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating OneHotEncoder object to transform integer categorical values into dummy categorical\n",
    "x = OneHotEncoder(categorical_features=[0]).fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets\n",
    "x_train,x_test,y_train,y_test,z_train,z_test = train_test_split(x,y,z, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
