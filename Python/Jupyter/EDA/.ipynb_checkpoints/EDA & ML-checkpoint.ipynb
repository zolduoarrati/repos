{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "* pip install lightgbm\n",
    "* conda install -c conda-forge xgboost\n",
    "* pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EDA & ML**  \n",
    "\n",
    "**Approaches and Techniques:**\n",
    "\n",
    "* EDA with Pandas and Seaborn\n",
    "* Find features with strong correlation to target variables questions and answers\n",
    "* Data preprocessing, converting categorical features mainly (country) to numerical\n",
    "* apply the basic Regression models of sklearn \n",
    "* use gridsearchCV to find the best parameters for each model\n",
    "* compare the performance of the Regressors and choose best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The notebook is organized as follows:**\n",
    "\n",
    "* **[Part 0: Imports, Settings and switches, Global functions](#Part-0-:-Imports,-Settings,-Functions)**  \n",
    "* import libraries  \n",
    "* settings for number of cross validations  \n",
    "* define functions that are used often\n",
    "\n",
    "* **[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)**  \n",
    "1.1 Get an overview of the features (numerical and categorical) and first look on the target variables questions and answers\n",
    "[shape, info, head and describe](#shape,-info,-head-and-describe)  \n",
    "[Distribution of the target variable Q](#The-target-variable-:-Distribution-of-questions-and-answers)  \n",
    "[Numerical and Categorical features](#Numerical-and-Categorical-features)  \n",
    "[List of features with missing values](#List-of-features-with-missing-values) and Filling missing values using [log transform](#log-transform)  \n",
    "1.2 Relation of all features to target questions and answers  \n",
    "[Seaborn regression plots for numerical features](#Plots-of-relation-to-target-for-all-numerical-features)  \n",
    "[List of numerical features and their correlation coefficient to target](#List-of-numerical-features-and-their-correlation-coefficient-to-target)  \n",
    "[Seaborn boxplots for categorical features](#Relation-to-questions-and-answers-for-all-categorical-features)  \n",
    "[List of categorical features and their unique values](#List-of-categorical-features-and-their-unique-values)  \n",
    "1.3 Determine the columns that show strong correlation to target  \n",
    "[Correlation matrix 1](#Correlation-matrix-1) : all numerical features determine features with largest correlation to questions and answers\n",
    "\n",
    "* **[Part 2: Data wrangling](#Part-2:-Data-wrangling)**  \n",
    "[Dropping all columns with weak correlation to questions and answers](#Dropping-all-columns-with-weak-correlation-to-questions-and-answers)  \n",
    "[Convert categorical columns to numerical](#Convert-categorical-columns-to-numerical)  \n",
    "[Checking correlation to questions and answers for the new numerical columns](#Checking-correlation-to-questions-and-answers-for-the-new-numerical-columns)  \n",
    "use only features with strong correlation to target  \n",
    "[Correlation Matrix 2 (including converted categorical columns)](#Correlation-Matrix-2-:-All-features-with-strong-correlation-to-questions-and-answers)  \n",
    "Create datasets for ML algorithms:                                                                          \n",
    "[OneHotEncoder](#OneHotEncoder)  \n",
    "[StandardScaler](#StandardScaler)\n",
    "\n",
    "* **[Part 3: Scikit-learn basic regression models and comparison of results](#Part-3:-Scikit-learn-basic-regression-models-and-comparison-of-results)**  \n",
    "implement GridsearchCV with RMSE metric for Hyperparameter tuning for these models from sklearn:  \n",
    "[Linear Regression](#Linear-Regression)  \n",
    "[Ridge](#Ridge)  \n",
    "[Lasso](#Lasso)  \n",
    "[Elastic Net](#Elastic-Net)  \n",
    "[Stochastic Gradient Descent](#SGDRegressor)  \n",
    "[DecisionTreeRegressor](#DecisionTreeRegressor)  \n",
    "[Random Forest Regressor](#RandomForestRegressor)  \n",
    "[KNN Regressor](#KNN-Regressor)  \n",
    "Baed on RMSE metric, compare performance of the regressors with their optimized parameters, then explore correlation of the predictions and make submission with mean of best models plot comparison:             \n",
    "[RMSE of all models](#Comparison-plot:-RMSE-of-all-models)  \n",
    "[Correlation of model results](#Correlation-of-model-results)  \n",
    "Mean of best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 : Imports, Settings, Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Lib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Math Lib for some statistics\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# df preprocessing Lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('max_columns', 105)\n",
    "\n",
    "# AI preprocessing lib\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "#import sklearn.impute.SimpleImputer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# ML Lib\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "# warning supressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#importing necessary models and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and switches\n",
    "\n",
    "* Here we can choose settings for optimal performance and runtime. \n",
    "* For example, nr_cv sets the number of cross validations used in GridsearchCV, and min_val_corr is the minimum value for the correlation coefficient to the target (only features with larger correlation will be used). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of cross validations used in the Model part \n",
    "nr_cv = 5\n",
    "\n",
    "# switch for using log values for questions, answers and features     \n",
    "use_logvals = 1    \n",
    "# target used for correlation \n",
    "target_1 = 'Questions_log'\n",
    "target_2 = 'Answers_log'    \n",
    "# only columns with correlation above this threshold value  \n",
    "# are used for the ML Regressors in Part 3\n",
    "min_val_corr = 0.4    \n",
    "    \n",
    "# switch for dropping columns that are similar to others already used and show a high correlation to these     \n",
    "drop_similar = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_score(grid):\n",
    "    \n",
    "    best_score = np.sqrt(-grid.best_score_)\n",
    "    print(best_score)    \n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_estimator_)\n",
    "    return best_score\n",
    "\n",
    "def print_cols_large_corr(df, nr_c, targ) :\n",
    "    corr = df.corr()\n",
    "    corr_abs = corr.abs()\n",
    "    print (corr_abs.nlargest(nr_c, targ)[targ])\n",
    "\n",
    "def plot_corr_matrix(df, nr_c, targ) :\n",
    "    \n",
    "    corr = df.corr()\n",
    "    corr_abs = corr.abs()\n",
    "    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n",
    "    cm = np.corrcoef(df[cols].values.T)\n",
    "\n",
    "    plt.figure(figsize=(nr_c/1.5, nr_c/1.5))\n",
    "    sns.set(font_scale=1.25)\n",
    "    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n",
    "                fmt='.2f', annot_kws={'size': 10}, \n",
    "                yticklabels=cols.values, xticklabels=cols.values\n",
    "               )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output, call \n",
    "print(check_output([\"dir\", \"C:\\\\Data\\\\all.csv\"],shell=True).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data into dataframe\n",
    "df =  pd.read_csv('C://Data/all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"List features with missing values and their percentage\")\n",
    "print(\"-\"*60)\n",
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Completely missing features (1.000000) will be eliminated as including them in the analysis will be useless as the rest of features will remain for now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtitute missing values by zero\n",
    "#df = df.fillna(0)\n",
    "## Displaying modified dataframe\n",
    "#df.head()\n",
    "## creating a variable vector\n",
    "# df = df.iloc[:,[0,6,7,8,9,10,11,12,13,14,15,16,17]]\n",
    "## Or\n",
    "# vec = df.loc[:,['Id','DisplayName','Location','country','AboutMe_length','activity_in_months','UpVotes','DownVotes','Reputation','Views','badges','Q_comments','A_comments','P_questions','P_answers']]\n",
    "df.drop(['ph_PostMigratedHere','ph_UnknownSuggestionEvent','ph_QuestionMerged','ph_PostMigrated','ph_PostMigrated','p_Wiki','ph_QuestionUnmerged','ph_PostDisassociated','ph_SuggestedEditApplied','ph_UnknownDevRelatedEvent','ph_VoteNullificationByDev','ph_PostTweeted','ph_PostUnmigrated','ph_UnknownModeratorEvent','ph_UnknownEvent','ph_CommentDiscussionMovedToChat','p_PrivilegeWiki','p_WikiPlaceholder'], axis= 1, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot of numerical variables\n",
    "* In this phase we will try to handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_cols = ['AboutMe_length','activity_in_months','UpVotes','DownVotes','Reputation','Views','badges','Q_comments','A_comments','P_questions','P_answers','p_ModeratorNomination','p_TagWiki','p_TagWikiExerpt','ph_InitialTitle','ph_EditTitle','ph_InitialBody','ph_InitialTags','ph_EditBody','ph_EditTags','ph_RollbackTitle','ph_RollbackBody','ph_PostReopened','ph_RollbackTags','ph_PostClosed','ph_PostDeleted','ph_PostUndeleted','ph_CommunityOwned','ph_PostLocked','ph_PostUnlocked','ph_QuestionUnprotected','ph_QuestionProtected','ph_PostNoticeRemoved','ph_PostNoticeAdded','ph_PostMergeSource','ph_PostMigratedAway','ph_PostMergeDestination']\n",
    "plt.figure(figsize=(30,20))\n",
    "df[num_cols].boxplot()\n",
    "plt.title(\"Numerical variables in our dataframe\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers using interquartile method\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['AboutMe_length','activity_in_months','UpVotes','DownVotes','Reputation','Views','badges','Q_comments','A_comments','P_questions','P_answers','p_ModeratorNomination','p_TagWiki','p_TagWikiExerpt','ph_InitialTitle','ph_EditTitle','ph_InitialBody','ph_InitialTags','ph_EditBody','ph_EditTags','ph_RollbackTitle','ph_RollbackBody','ph_PostReopened','ph_RollbackTags','ph_PostClosed','ph_PostDeleted','ph_PostUndeleted','ph_CommunityOwned','ph_PostLocked','ph_PostUnlocked','ph_QuestionUnprotected','ph_QuestionProtected','ph_PostNoticeRemoved','ph_PostNoticeAdded','ph_PostMergeSource','ph_PostMigratedAway','ph_PostMergeDestination']\n",
    "plt.figure(figsize=(30,20))\n",
    "df[num_cols].boxplot()\n",
    "plt.title(\"Numerical variables in our dataframe\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View each column seperately\n",
    "\n",
    "# num_cols = ['Reputation']\n",
    "# plt.figure(figsize=(20,15))\n",
    "# df[num_cols].boxplot()\n",
    "# plt.title(\"Numerical variables in our dataframe\", fontsize=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Reputation > 600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Views > 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.P_answers > 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.DownVotes > 30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.at[[77574,82886,84878,85942],'Reputation'] = None;\n",
    "# df.at[[77574,84878,85942],'Views'] = None;\n",
    "# df.at[[77574],'P_answers'] = None;\n",
    "# df.at[[85524],'DownVotes'] = None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets\n",
    "df_train,df_test= train_test_split(df, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Overview of features and relation to target\n",
    "\n",
    "Let's get a first overview of the train and test dataset\n",
    "* How many rows and columns are there?  \n",
    "* What are the names of the features (columns)?  \n",
    "* Which features are numerical, which are categorical?  \n",
    "* How many values are missing?  \n",
    "The **shape** and **info** methods answer these questions. Whereas, the **head** displays some rows of the dataset **describe** gives a summary of the statistics (only for numerical columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape, Info, Head & Describe -----> Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*100)\n",
    "print('training sample size')\n",
    "print(df_train.shape)\n",
    "print('-'*100)\n",
    "print('testing sample size')\n",
    "print(df_test.shape)\n",
    "print('-'*100)\n",
    "print('training sample features description')\n",
    "print(df_train.info())\n",
    "print('-'*100)\n",
    "print('testing sample features description')\n",
    "print(df_train.info())\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems like the trainning and tesing dataframe *(df)* vector *(vec)* consists of 39 columns (38 features excluding Id), as for the training df vec, it has 112147 entries (number of rows). On the other hand,  df test vec has 28037 entries.  \n",
    "* There are lots of info that is probably related to the dependent variables - > target (questions and answers) such as badges, reputaion, etc...   \n",
    "* Maybe other features are not so important for predicting the target, also there might be a strong correlation for some of the features (like activity_in_month).\n",
    "* There are missing in some columns and it seems some countries tend to have more missing data than others, we are going to deal with missing data accordingly in a later stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying a sample from the training dataframe\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying a descriptive stats regarding the training dataframe \n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying a sample from the testing dataframe\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying a descriptive stats regarding the testing dataframe\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of target variables (Questions and Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimising plots size\n",
    "# data = np.random.normal(0, 1, 3)\n",
    "# array([-1.18878589,  0.59627021,  1.59895721])\n",
    "# ploty = plt.figure(figsize=(20, 15))\n",
    "# sns.boxplot(x=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ conversion error prevention **\n",
    "# df_train = df_train.fillna(0)\n",
    "# Seaborn 0_0\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.distplot(df_train['P_questions'].dropna());\n",
    "# skewness and Kurtosis\n",
    "print(\"Skewness: %f\" % df_train['P_questions'].skew());\n",
    "print(\"Kurtosis: %f\" % df_train['P_questions'].kurt());\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "# ValueError: cannot convert float NaN to integer --- Error --- just drop NaN or convertion is required ---> utilising fillna in early phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('-'*100)\n",
    "print('-'*100)\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.distplot(df_train['P_answers'].dropna());\n",
    "# skewness and Kurtosis\n",
    "print(\"Skewness: %f\" % df_train['P_answers'].skew());\n",
    "print(\"Kurtosis: %f\" % df_train['P_answers'].kurt());\n",
    "print('-'*100)\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see, the target variable for both questions and answers is not normally distributed. \n",
    "* This behaviour can leads to performance reduction in the ML regression modeling due the fact that some models assume normal distribution.\n",
    "* Therfore a log transformation is required (ref: see sklearn info on preprocessing) to enhance distribution visualisation.\n",
    "* Before running log transformation, we need to deal with zero values in questions and answers otherwise will face the error due to zero division which will produce -1nf (minus infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide = 'ignore')\n",
    "df_train['Questions_log'] = np.where(df_train.loc[:,['P_questions']]>0, np.log(df_train.loc[:,['P_questions']]), 0)\n",
    "df_train['Answers_log'] = np.where(df_train.loc[:,['P_answers']]>0, np.log(df_train.loc[:,['P_answers']]), 0)\n",
    "\n",
    "df_test['Questions_log'] = np.where(df_test.loc[:,['P_questions']]>0, np.log(df_test.loc[:,['P_questions']]), 0)\n",
    "df_test['Answers_log'] = np.where(df_test.loc[:,['P_answers']]>0, np.log(df_test.loc[:,['P_answers']]), 0)\n",
    "\n",
    "# # alternative implementation to avoids warnings.\n",
    "# loc = np.where(myarray>0)\n",
    "# result2 = np.zeros_like(myarray, dtype=float)\n",
    "# result2[loc] =np.log(myarray[loc])\n",
    "\n",
    "# # answer\n",
    "# myarray= np.random.randint(10,size=10)\n",
    "# result = np.where(myarray>0, np.log(myarray), 0)\n",
    "\n",
    "# # check it is giving right solution:\n",
    "# print(np.allclose(result, result2))\n",
    "\n",
    "# append columns\n",
    "# df_train = pd.concat([df_train, Questions_log], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train['Questions_log'].dropna());\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train['Answers_log'].dropna());\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*100)\n",
    "print(\"Skewness and kurtosis for questions log\")\n",
    "print(\"Skewness: %f\" % df_train['Questions_log'].skew());\n",
    "print(\"Kurtosis: %f\" % df_train['Questions_log'].kurt());\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*100)\n",
    "print(\"Skewness and kurtosis for answers log\")\n",
    "print(\"Skewness: %f\" % df_train['Answers_log'].skew());\n",
    "print(\"Kurtosis: %f\" % df_train['Questions_log'].kurt());\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping old features from the loaded dataframe\")\n",
    "df_train.drop(\"P_questions\",axis=1,inplace=True);\n",
    "df_train.drop(\"P_answers\",axis=1,inplace=True);\n",
    "df_test.drop(\"P_questions\",axis=1,inplace=True);\n",
    "df_test.drop(\"P_answers\",axis=1,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*100)\n",
    "print(\"Counting numerical and categorical features exist in the dataframe\")\n",
    "print(\"-\"*100)\n",
    "numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n",
    "print(\"Number of numerical features: \", len(numerical_feats));\n",
    "categorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\n",
    "print(\"Number of categorical features: \", len(categorical_feats));\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*35)\n",
    "print(\"Displaying numerical features\")\n",
    "print(\"-\"*35)\n",
    "print(df_train[numerical_feats].columns)\n",
    "print(\"-\"*35)\n",
    "print(\"Displaying categorical features\")\n",
    "print(\"-\"*35)\n",
    "print(df_train[categorical_feats].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"Displaying numerical features dataframe\")\n",
    "print(\"-\"*40)\n",
    "df_train[numerical_feats].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"Displaying categorical features\")\n",
    "print(\"-\"*40)\n",
    "df_train[categorical_feats].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"List features with missing values and their percentage\")\n",
    "print(\"-\"*40)\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling missing values\n",
    "* we have a few columns with a large percentage of NaN entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_fillna = ['A_comments','Q_comments','AboutMe_length']\n",
    "# # replace 'NaN' with zero in these columns\n",
    "# for col in cols_fillna:\n",
    "#     df_train[col].fillna(0,inplace=True)\n",
    "#     df_test[col].fillna(0,inplace=True)\n",
    "#################################################\n",
    "# subtitute missing values by zero\n",
    "df_train = df_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"List features after handeling missing values\")\n",
    "print(\"-\"*40)\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"Missing values in train data ?\")\n",
    "print(\"-\"*40)\n",
    "df_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"Missing values in test data ?\")\n",
    "print(\"-\"*40)\n",
    "df_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log transform\n",
    "\n",
    "* Like the target features, other features are not normally distributed therefore it is better to tranform those features using log in df_train and df_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"Checking for skewness and kurtosis:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for col in numerical_feats:\n",
    "    print('{:25}'.format(col),'    ', \n",
    "          'Skewness: {:05.2f}'.format(df_train[col].skew()) , \n",
    "          '-------->' ,\n",
    "          'Kurtosis: {:05.2f}'.format(df_train[col].kurt())  \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_train.dtypes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_train.select_dtypes(include = ['float64', 'int64'])\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Relation of features to target (Questions_log and Answers_log)\n",
    "### Plots of relation to target for all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr_rows = 12\n",
    "nr_cols = 3\n",
    "\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n",
    "\n",
    "li_num_feats = list(numerical_feats)\n",
    "li_not_plot = ['Id', 'Questions_log']\n",
    "li_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n",
    "\n",
    "\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):  \n",
    "        i = r*nr_cols+c\n",
    "        if i < len(li_plot_num_feats):\n",
    "            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target_1], ax = axs[r][c])\n",
    "            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target_1])\n",
    "            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n",
    "            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n",
    "            axs[r][c].set_title(str_title,fontsize=11)\n",
    "            \n",
    "plt.tight_layout()    \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr_rows = 12\n",
    "nr_cols = 3\n",
    "\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n",
    "\n",
    "li_num_feats = list(numerical_feats)\n",
    "li_not_plot = ['Id', 'Answers_log']\n",
    "li_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n",
    "\n",
    "\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):  \n",
    "        i = r*nr_cols+c\n",
    "        if i < len(li_plot_num_feats):\n",
    "            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target_2], ax = axs[r][c])\n",
    "            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target_2])\n",
    "            #axs[r][c].text(0.4,0.9,\"title\",fontsize=7)\n",
    "            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n",
    "            axs[r][c].set_title(str_title,fontsize=11)\n",
    "            \n",
    "plt.tight_layout()    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of numerical features and their correlation coefficient to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find columns with strong correlation to target**  \n",
    "Only those with r > min_val_corr are used in the ML Regressors in Part 3  \n",
    "The value for min_val_corr can be chosen in global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train.corr()\n",
    "corr_abs = corr.abs()\n",
    "\n",
    "nr_num_cols = len(numerical_feats)\n",
    "ser_corr = corr_abs.nlargest(nr_num_cols, target_1)[target_1]\n",
    "\n",
    "cols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\n",
    "cols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ser_corr)\n",
    "print(\"-\"*100)\n",
    "print(\"List of numerical features with r above min_val_corr :\")\n",
    "print(cols_abv_corr_limit)\n",
    "print(\"-\"*100)\n",
    "print(\"List of numerical features with r below min_val_corr :\")\n",
    "print(cols_bel_corr_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train.corr()\n",
    "corr_abs = corr.abs()\n",
    "\n",
    "nr_num_cols = len(numerical_feats)\n",
    "ser_corr = corr_abs.nlargest(nr_num_cols, target_2)[target_2]\n",
    "\n",
    "cols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\n",
    "cols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ser_corr)\n",
    "print(\"-\"*100)\n",
    "print(\"List of numerical features with r above min_val_corr :\")\n",
    "print(cols_abv_corr_limit)\n",
    "print(\"-\"*100)\n",
    "print(\"List of numerical features with r below min_val_corr :\")\n",
    "print(cols_bel_corr_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of categorical features and their unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*100)\n",
    "for catg in list(categorical_feats) :\n",
    "    print(df_train[catg].value_counts())\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Questions and Answers for all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_cat_feats = list(categorical_feats)\n",
    "nr_rows = 2\n",
    "nr_cols = 2\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*5,nr_rows*4))\n",
    "\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):  \n",
    "        i = r*nr_cols+c\n",
    "        if i < len(li_cat_feats):\n",
    "            sns.boxplot(x=li_cat_feats[i], y=target_1, data=df_train, ax = axs[r][c])\n",
    "plt.tight_layout()    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_cat_feats = list(categorical_feats)\n",
    "nr_rows = 2\n",
    "nr_cols = 2\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*5,nr_rows*4))\n",
    "\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):  \n",
    "        i = r*nr_cols+c\n",
    "        if i < len(li_cat_feats):\n",
    "            sns.boxplot(x=li_cat_feats[i], y=target_2, data=df_train, ax = axs[r][c])\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix 1\n",
    "**Features with largest correlation to Questions_Log**  \n",
    "all numerical features with correlation coefficient above threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_feats = len(cols_abv_corr_limit)\n",
    "plot_corr_matrix(df_train, nr_feats, target_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix 1\n",
    "**Features with largest correlation to Answers_Log**  \n",
    "all numerical features with correlation coefficient above threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_feats = len(cols_abv_corr_limit)\n",
    "plot_corr_matrix(df_train, nr_feats, target_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Of those features with the largest correlation to questions and answers, some also are correlated strongly to each other.\n",
    "* To avoid failures of the ML regression models due to multicollinearity, these are dropped in part 2.\n",
    "* This is optional and controlled by the switch drop_similar (global settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data wrangling\n",
    "\n",
    "* Drop all columns with only small correlation to P_questions and P_answers\n",
    "* Transform Categorical to numerical\n",
    "* Handling columns with missing data if it is needed\n",
    "* Log values\n",
    "* Drop all columns with strong correlation to similar features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping all columns with weak correlation to Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = df_test['Id']\n",
    "\n",
    "to_drop_num  = cols_bel_corr_limit\n",
    "# to_drop_catg = catg_weak_corr\n",
    "\n",
    "cols_to_drop = ['Id'] + to_drop_num #+ to_drop_catg \n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df.drop(cols_to_drop, inplace= True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical columns to numerical  \n",
    "* We need to transform the country categorcial feature to numerical.\n",
    "* To investigate the relation of the categorcial feature to target in more detail, we will create violinplots\n",
    "* Also, we look at the mean of Questions and Answers as function of category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have one categorical value then we will take it as it is\n",
    "catg_strong_corr = [ 'country']\n",
    "catg_list = catg_strong_corr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \"*43+\"Violinplots btween Questions and Country\")\n",
    "print(\"-\"*127)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 5)\n",
    "sns.violinplot(x='country', y=target_1, data=df_train, ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \"*43+\"Violinplots btween Answers and Country\")\n",
    "print(\"-\"*127)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 5)\n",
    "sns.violinplot(x='country', y=target_2, data=df_train, ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for catg in catg_list :\n",
    "    g1 = df_train.groupby(catg)[target_1].mean()\n",
    "    g2 = df_train.groupby(catg)[target_2].mean()\n",
    "    print(g1)\n",
    "    print(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to copy a country column then transform countries into integers and this can be done in different ways\n",
    "# 'country'\n",
    "country_catg2 = ['usa']\n",
    "country_catg3 = ['china']\n",
    "country_catg4 = ['russia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_test]:   \n",
    "    df['country_num'] = 1  \n",
    "    df.loc[(df['country'].isin(country_catg2) ), 'country_num'] = 2\n",
    "    df.loc[(df['country'].isin(country_catg3) ), 'country_num'] = 3\n",
    "    df.loc[(df['country'].isin(country_catg4) ), 'country_num'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking correlation to questions and answers for the new numerical columnsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_num = ['country_num']\n",
    "\n",
    "nr_rows = 2\n",
    "nr_cols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n",
    "\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):  \n",
    "        i = r*nr_cols+c\n",
    "        if i < len(new_col_num):\n",
    "            sns.regplot(df_train[new_col_num[i]], df_train[target_1], ax = axs[r][c])\n",
    "            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target_1])\n",
    "            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n",
    "            axs[r][c].set_title(str_title,fontsize=11)\n",
    "            \n",
    "plt.tight_layout()    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_num = ['country_num']\n",
    "\n",
    "nr_rows = 2\n",
    "nr_cols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n",
    "\n",
    "for r in range(0,nr_rows):\n",
    "    for c in range(0,nr_cols):  \n",
    "        i = r*nr_cols+c\n",
    "        if i < len(new_col_num):\n",
    "            sns.regplot(df_train[new_col_num[i]], df_train[target_2], ax = axs[r][c])\n",
    "            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target_2])\n",
    "            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n",
    "            axs[r][c].set_title(str_title,fontsize=11)\n",
    "            \n",
    "plt.tight_layout()    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dublicate column\n",
    "dup = df_train['country_num']\n",
    "# df_train['country_dum'] = df_train['country']\n",
    "# df_train.head()\n",
    "# # check data types \n",
    "# df_train[\"country_dum\"] = df_train[\"country_dum\"].astype('category')\n",
    "# df_train.dtypes\n",
    "# # create a new categorical dummy var from original\n",
    "# df_train[\"country_dum1\"] = df_train[\"country_dum\"].cat.codes\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the categorical conversion is correct, first convert the created country_num into list then loop through the list to see if the numbers adds up\n",
    "cate = df_train['country_num'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check numerical values for the categorical feature have changed correctly\n",
    "print('-'*100)\n",
    "usa = 0\n",
    "china = 0\n",
    "russia = 0\n",
    "for column in cate:\n",
    "    if column == 2: \n",
    "        usa +=1\n",
    "    elif column == 3:\n",
    "        china +=1\n",
    "    else:\n",
    "        russia+=4\n",
    "print(\"usa = \",usa)\n",
    "print(\"china = \",china)\n",
    "print(\"russia = \",russia)\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropping the converted categorical column and the new numerical columns with weak correlation\n",
    "* These will probably be useful for optimal performance of the Regressors in part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catg_cols_to_drop = ['country']\n",
    "\n",
    "corr1 = df_train.corr()\n",
    "corr_abs_1 = corr1.abs()\n",
    "\n",
    "nr_all_cols = len(df_train)\n",
    "ser_corr_1 = corr_abs_1.nlargest(nr_all_cols, target_1)[target_1]\n",
    "ser_corr_2 = corr_abs_1.nlargest(nr_all_cols, target_2)[target_2]\n",
    "print(\"-\"*100)\n",
    "print(ser_corr_1)\n",
    "print(\"-\"*50)\n",
    "print(ser_corr_2)\n",
    "print(\"-\"*100)\n",
    "\n",
    "cols_bel_corr_limit_1 = list(ser_corr_1[ser_corr_1.values <= min_val_corr].index)\n",
    "cols_bel_corr_limit_2 = list(ser_corr_2[ser_corr_2.values <= min_val_corr].index)\n",
    "\n",
    "# for df in [df_train, df_test] :\n",
    "#     df.drop(catg_cols_to_drop, inplace= True, axis = 1)\n",
    "#     df.drop(cols_bel_corr_limit_1, inplace= True, axis = 1)    \n",
    "#     df.drop(cols_bel_corr_limit_2, inplace= True, axis = 1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems like country_num is weakly correlated with both Questions_log and Answers_log. However, the feature UpVotes is weakly correlated with Questions_log but this is not the case with Answers_log. Thus, we will keep it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['country','country_num'], inplace= True, axis = 1)\n",
    "df_test.drop(['country','country_num'], inplace= True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr2 = df_train.corr()\n",
    "corr_abs_2 = corr2.abs()\n",
    "\n",
    "nr_all_cols = len(df_train)\n",
    "ser_corr_3 = corr_abs_2.nlargest(nr_all_cols, target_1)[target_1]\n",
    "ser_corr_4 = corr_abs_2.nlargest(nr_all_cols, target_2)[target_2]\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(ser_corr_3)\n",
    "print(\"-\"*50)\n",
    "print(ser_corr_4)\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix 2 : \n",
    "* All features with strong correlation to Questions_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_feats=len(df_train.columns)\n",
    "plot_corr_matrix(df_train, nr_feats, target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_feats=len(df_train.columns)\n",
    "plot_corr_matrix(df_train, nr_feats, target_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of samples into the train data is {}.'.format(df_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of samples into the test data is {}.'.format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = corr_abs.nlargest(nr_all_cols, target)[target].index\n",
    "cols = list(cols)\n",
    "\n",
    "if drop_similar == 1 :\n",
    "    for col in ['activity_in_months','1stFlrSF','TotRmsAbvGrd','GarageYrBlt'] :\n",
    "        if col in cols: \n",
    "            cols.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the independent variable vector\n",
    "# x = df.iloc[:,6:15].values\n",
    "# # Question dependent variable vector \n",
    "# y = df.iloc[:, -2:-1].values\n",
    "# # Answer dependent variable vector \n",
    "# z = df.iloc[:,-1:].values\n",
    "# # create labelEncoder object to transform categorical values into integers\n",
    "# x[:, 0] = LabelEncoder().fit_transform(x[:, 0])\n",
    "# y = LabelEncoder().fit_transform(y)\n",
    "# z = LabelEncoder().fit_transform(z)\n",
    "# # creating OneHotEncoder object to transform integer categorical values into dummy categorical\n",
    "# x = OneHotEncoder(categorical_features=[0]).fit_transform(x).toarray()\n",
    "# # Create training and testing sets\n",
    "# x_train,x_test,y_train,y_test,z_train,z_test = train_test_split(x,y,z, test_size = 0.2, random_state = 0)\n",
    "# # feature scaling\n",
    "# sc_x = StandardScaler()\n",
    "# x_train = sc_x.fit_transform(x_train)\n",
    "# x_test = sc_x.transform(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
